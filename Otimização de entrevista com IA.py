# -*- coding: utf-8 -*-
"""C√≥pia de Conhe√ßa o Colab

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aWrq1enNDqNz_BWxGRuRXlc0BwnjhrF3
"""

from google.colab import files

# Execute e selecione o arquivo .json correto do seu computador
uploaded = files.upload()

import pandas as pd
import json

# --- ETAPA 1: Transformar 'aplicacoes.json' ---
with open('applicants.json', 'r', encoding='utf-8') as f:
    aplicacoes_raw = json.load(f)

aplicacoes_df = pd.json_normalize(aplicacoes_raw).T.reset_index()
aplicacoes_df[['id_aplicacao', 'campo']] = aplicacoes_df['index'].str.extract(r"^(\d+)\.(.*)")
aplicacoes_df = aplicacoes_df.pivot(index='id_aplicacao', columns='campo', values=0).reset_index()

# --- ETAPA 2: Transformar 'vagas.json' ---
with open('vagas.json', 'r', encoding='utf-8') as f:
    vagas_raw = json.load(f)

vagas_df = pd.json_normalize(vagas_raw).T.reset_index()
vagas_df[['id_vaga', 'campo']] = vagas_df['index'].str.extract(r"^(\d+)\.(.*)")
vagas_df = vagas_df.pivot(index='id_vaga', columns='campo', values=0).reset_index()

# --- ETAPA 3: Tentar detectar automaticamente a coluna de v√≠nculo ---
possiveis_chaves_aplicacao = [col for col in aplicacoes_df.columns if 'vaga' in col.lower() or 'codigo' in col.lower()]
possiveis_chaves_vaga = [col for col in vagas_df.columns if 'id' in col.lower() or 'codigo' in col.lower() or 'vaga' in col.lower()]

print("üîç Poss√≠veis chaves em 'aplicacoes':", possiveis_chaves_aplicacao)
print("üîç Poss√≠veis chaves em 'vagas':", possiveis_chaves_vaga)

# Tentar encontrar uma chave em comum (valores coincidentes)
chave_encontrada = None
for col_ap in possiveis_chaves_aplicacao:
    for col_vg in possiveis_chaves_vaga:
        if aplicacoes_df[col_ap].isin(vagas_df[col_vg]).any():
            chave_encontrada = (col_ap, col_vg)
            break
    if chave_encontrada:
        break

if chave_encontrada:
    col_ap, col_vg = chave_encontrada
    print(f"\n‚úÖ Chave encontrada: aplicacoes['{col_ap}'] ‚Üî vagas['{col_vg}']")
    df_final = aplicacoes_df.merge(vagas_df, how='left', left_on=col_ap, right_on=col_vg, suffixes=('', '_vaga'))
    print(f"\nüü¢ Merge realizado com sucesso. Shape final: {df_final.shape}")
else:
    print("\n‚ùå Nenhuma chave comum encontrada para merge. Verifique os dados manualmente.")

print(df_final.columns.tolist())

[col for col in df_final.columns if 'status' in col.lower()]

[col for col in df_final.columns if any(palavra in col.lower() for palavra in ['avaliacao', 'resultado', 'entrevista', 'motivo', 'decisao', 'andamento', 'final'])]

colunas = [
    'formacao_e_idiomas.nivel_academico',
    'formacao_e_idiomas.nivel_ingles',
    'informacoes_profissionais.nivel_profissional',
    'informacoes_profissionais.remuneracao',
    'perfil_vaga.competencia_tecnicas_e_comportamentais',
    'informacoes_basicas.tipo_contratacao',
    'informacoes_basicas.cliente',
    'informacoes_basicas.data_final'  # usada como base para target
]

df_modelo = df_final[colunas].copy()

# Criar vari√°vel-alvo com base na presen√ßa de data_final
df_modelo['target'] = df_modelo['informacoes_basicas.data_final'].notna().astype(int)

# Remover a coluna usada como base para o target
df_modelo = df_modelo.drop(columns=['informacoes_basicas.data_final'])

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report
import joblib

X = df_modelo.drop('target', axis=1)
y = df_modelo['target']

le_dict = {}
for col in X.columns:
    if X[col].dtype == 'object':
        le = LabelEncoder()
        X[col] = le.fit_transform(X[col].astype(str))
        le_dict[col] = le

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

model = RandomForestClassifier(random_state=42)
model.fit(X_train, y_train)

y_pred = model.predict(X_test)
print(classification_report(y_test, y_pred))

# Salvar modelo e recursos
joblib.dump(model, 'modelo_entrevista.pkl')
joblib.dump(le_dict, 'label_encoders.pkl')
joblib.dump(list(X.columns), 'features_utilizadas.pkl')

import joblib

model = joblib.load("modelo_entrevista.pkl")
le_dict = joblib.load("label_encoders.pkl")
features = joblib.load("features_utilizadas.pkl")

from fastapi import FastAPI
from pydantic import BaseModel
import joblib
import pandas as pd

# Carregar o modelo e os encoders
model = joblib.load("modelo_entrevista.pkl")
le_dict = joblib.load("label_encoders.pkl")
features = joblib.load("features_utilizadas.pkl")

# Inicializar a API
app = FastAPI(title="API Previs√£o de Entrevista")

# Definir o esquema de entrada
class Candidato(BaseModel):
    formacao_e_idiomas_nivel_academico: str
    formacao_e_idiomas_nivel_ingles: str
    informacoes_profissionais_nivel_profissional: str
    informacoes_profissionais_remuneracao: float
    perfil_vaga_competencia_tecnicas_e_comportamentais: str
    informacoes_basicas_tipo_contratacao: str
    informacoes_basicas_cliente: str

@app.post("/prever/")
def prever_conclusao(candidato: Candidato):
    # Criar DataFrame com os dados recebidos
    dados = pd.DataFrame([candidato.dict()])
    dados.columns = features  # garantir ordem correta

    # Aplicar LabelEncoding nas colunas categ√≥ricas
    for col in dados.columns:
        if col in le_dict:
            dados[col] = le_dict[col].transform(dados[col].astype(str))

    # Fazer a previs√£o
    pred = model.predict(dados)[0]
    prob = model.predict_proba(dados)[0][pred]

    return {
        "previsao": int(pred),
        "probabilidade": round(prob, 2),
        "mensagem": "Entrevista prov√°vel de ser conclu√≠da" if pred == 1 else "Entrevista n√£o deve ser conclu√≠da"
    }

import nest_asyncio
from pyngrok import ngrok
import uvicorn

nest_asyncio.apply()

public_url = ngrok.connect(8000)
print(f"URL p√∫blica: {public_url}/docs")

uvicorn.run(app, host="0.0.0.0", port=8000)

dockerfile = """
FROM python:3.9-slim

WORKDIR /app

COPY . .

RUN pip install --no-cache-dir -r requirements.txt

CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]
"""

with open("Dockerfile", "w") as f:
    f.write(dockerfile)

# Ver conte√∫do
!cat Dockerfile

!pip install fastapi nest-asyncio pyngrok uvicorn

!pip install pytest

# Recriar o arquivo de testes unit√°rios com tratamento robusto para valores n√£o vistos

test_model_code = """\
import joblib
import pandas as pd

def test_model_carrega():
    model = joblib.load('modelo_entrevista.pkl')
    assert model is not None

def test_previsao_simples():
    model = joblib.load('modelo_entrevista.pkl')
    features = joblib.load('features_utilizadas.pkl')
    le_dict = joblib.load('label_encoders.pkl')

    dados = {
        'formacao_e_idiomas.nivel_academico': 'Ensino M√©dio',
        'formacao_e_idiomas.nivel_ingles': 'Intermedi√°rio',
        'informacoes_profissionais.nivel_profissional': 'J√∫nior',
        'informacoes_profissionais.remuneracao': 3500,
        'perfil_vaga.competencia_tecnicas_e_comportamentais': 'HTML, CSS',
        'informacoes_basicas.tipo_contratacao': 'CLT',
        'informacoes_basicas.cliente': 'Empresa XP'
    }

    df = pd.DataFrame([dados])
    for col in df.columns:
        if col in le_dict:
            encoder = le_dict[col]
            try:
                df[col] = encoder.transform([df[col][0]])
            except ValueError:
                df[col] = encoder.transform([encoder.classes_[0]])

    df = df[features]
    pred = model.predict(df)[0]
    assert pred in [0, 1]
"""

with open("test_model.py", "w") as f:
    f.write(test_model_code)

"/content/test_model.py criado com sucesso."

!pytest test_model.py

import logging
logging.basicConfig(level=logging.INFO)